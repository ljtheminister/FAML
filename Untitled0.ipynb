{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import random\n",
      "from numpy import sqrt\n",
      "from activation_functions import *\n",
      "from cost_functions import *\n",
      "from numpy.linalg import norm\n",
      "\n",
      "class NeuralNetwork:\n",
      "    def __init__(self, X, y, layers, alpha=0.01, beta=0.0 ,test_prop=0.9, seed_parameter=1, activation_function='logistic', loss_function='squared_loss', output_function='linear'):\n",
      "\n",
      "        self.X = X\n",
      "        self.y = y\n",
      "        self.N, self.P = X.shape\n",
      "\n",
      "        self.seed = seed_parameter\n",
      "        self.layers = layers\n",
      "\n",
      "        self.N_train = np.floor(self.N*(1-test_prop))\n",
      "        self.N_test = self.N - self.N_train\n",
      "\n",
      "        row_idx = [i for i in xrange(self.N)]\n",
      "        random.shuffle(row_idx)\n",
      "\n",
      "        '''\n",
      "        self.X_train = self.X[row_idx[:self.N_train],:]\n",
      "        self.X_test = self.X[row_idx[self.N_train:],:]\n",
      "\n",
      "        self.y_train = self.y[row_idx[:self.N_train],:]\n",
      "        self.y_test = self.y[row_idx[self.N_train:],:]\n",
      "        '''\n",
      "\n",
      "        self.W = {} # weights\n",
      "        self.b = {} # biases\n",
      "        self.alpha = alpha # learning rate\n",
      "        self.beta = beta #learning rate for bias\n",
      "\n",
      "        self.activation_function, self.activation_gradient = get_activation_function(activation_function)\n",
      "        self.loss_function = get_cost_function(loss_function)\n",
      "        self.output_function, self.output_gradient = get_activation_function(output_function)\n",
      "\n",
      "    def normalization(X):\n",
      "        P = X.shape[1]\n",
      "        for p in xrange(P):\n",
      "            mean= np.mean(X[:,p])\n",
      "            variance = np.variance(X[:,p])\n",
      "            X[:,p]= (X[:,p] - mean)/variance\n",
      "        return X\n",
      "\n",
      "    def w_initial(self, input, output):\n",
      "        return sqrt(6.0/(input+output))\n",
      "\n",
      "    def initialize_weights(self):\n",
      "        inputs = self.P\n",
      "        outputs = self.layers[0]\n",
      "        w = self.w_initial(inputs, outputs)\n",
      "        self.W[0] = np.random.uniform(-w, w, size=(inputs, outputs))\n",
      "        self.b[0] = 0\n",
      "\n",
      "        for i in xrange(1, len(self.layers)):\n",
      "            inputs = self.layers[i-1]\n",
      "            outputs = self.layers[i]\n",
      "            w = self.w_initial(inputs, outputs)\n",
      "            self.W[i] = np.random.uniform(-w, w, size=(inputs, outputs))\n",
      "            self.b[i] = 0\n",
      "\n",
      "    def compute_loss(self, X, y):\n",
      "        return self.loss_function(X,y)\n",
      "\n",
      "    def compute_error(self, X, y):\n",
      "        return y-X\n",
      "\n",
      "\n",
      "    def feed_forward(self, X):\n",
      "        N = X.shape[0]\n",
      "        self.W_length = len(self.W)\n",
      "        z = {}\n",
      "        z_new = X\n",
      "\n",
      "        for layer_idx in xrange(self.W_length):\n",
      "            z[layer_idx] = z_new\n",
      "            P = self.W[layer_idx].shape[1]\n",
      "            z_new = np.zeros((N,P))\n",
      "            for p in xrange(P):\n",
      "                z_new[:, p] = self.activation_function(z[layer_idx].dot(self.W[layer_idx][:,p]) + np.ones(N)*self.b[layer_idx])\n",
      "        #outer layer\n",
      "        P = self.W[self.W_length-1].shape[1]\n",
      "        for p in xrange(P):\n",
      "            #z_new[:, p] = self.output_function(z[layer_idx], self.W[self.W_length-1][:,p], self.b[self.W_length-1])\n",
      "            z_new[:, p] = z[layer_idx].dot(self.W[self.W_length-1][:,p]) + self.b[self.W_length-1]\n",
      "        z[self.W_length] = z_new\n",
      "        return z\n",
      "    \n",
      "    def back_propagation(self, X, y, z):\n",
      "        batch_error = list(self.compute_error(z[self.W_length], np.array([y]).T))\n",
      "        # update step for output layer (linear update)\n",
      "        if self.W_length-1 !=0:\n",
      "            for i,e in enumerate(batch_error):                            \n",
      "                print type(batch_error)\n",
      "                #print e\n",
      "                #print z[self.W[self.W_length-1][i,:].shape\n",
      "                #delta = e*(z[self.W_length-1][i,:])\n",
      "                print 'e-shape', e.shape\n",
      "                print 'z-shape', z[self.W_length-1].shape\n",
      "                delta = e*(z[self.W_length-1][i,:].T)\n",
      "                print delta.shape\n",
      "                self.W[self.W_length-1] += self.alpha*delta\n",
      "                self.b[self.W_length-1] += self.beta*e\n",
      "\n",
      "                # backprop for inside layers\n",
      "                for layer_idx in xrange(self.W_length-2, 0, -1):\n",
      "                    len_z = len(z[layer_idx][i,:])\n",
      "                    len_W_col=len(z[layer_idx][i,:].dot(self.W[layer_idx]))\n",
      "                    self.W[layer_idx] += self.alpha*e*z[layer_idx][i,:].reshape((len_z,1)).dot(self.activation_gradient(z[layer_idx][i,:].dot(self.W[0])).reshape((1,len_W_col)))\n",
      "                    self.b[layer_idx] += self.beta*e\n",
      "\n",
      "                len_W_col=len(z[0][i,:].dot(self.W[0]))\n",
      "                len_z = len(z[0][i,:])\n",
      "                self.W[0] += self.alpha*e*z[0][i,:].reshape((len_z,1)).dot(self.activation_gradient(z[0][i,:].dot(self.W[0])).reshape((1,len_W_col)))\n",
      "                self.b[0] += self.beta*e\n",
      "        else:\n",
      "            for i,e in enumerate(batch_error):\n",
      "                delta = (e*z[self.W_length-1][i,:])\n",
      "                self.W[self.W_length-1] += self.alpha*np.array([delta]).T\n",
      "                self.b[self.W_length-1] += self.beta*e\n",
      "\n",
      "\n",
      "\n",
      "    #rbm training with real values input\n",
      "    def RBM_positive(self, X):\n",
      "        h=self.feed_forward(X)\n",
      "        return h[1]\n",
      "\n",
      "    def RBM_negative(self,h):\n",
      "        N=h.shape[0]\n",
      "        P = self.W[0].shape[0]\n",
      "        v1 = np.zeros((N,P))\n",
      "        v1 = h.dot((self.W[0]).T) + np.ones((N,P))*self.b[0]\n",
      "        return v1\n",
      "\n",
      "    def RBM_update(self, v , v1, h ,h1):\n",
      "        self.W[0]+= self.alpha*( (v.T).dot(h) - (v1.T).dot(h1) )\n",
      "        return self.W[0]\n",
      "\n",
      "\n",
      "\n",
      "    def mainNN(self,X,y):\n",
      "        self.initialize_weights()\n",
      "\n",
      "        #Neural network training\n",
      "        for i in xrange(15):\n",
      "            z = self.feed_forward(self.X)\n",
      "            print 0.5*norm(z[self.W_length]-y)**2\n",
      "            self.back_propagation(X, y, z)\n",
      "            print self.W\n",
      "\n",
      "    def mainRBM(self,X):\n",
      "        self.initialize_weights()\n",
      "\n",
      "        #RBM training\n",
      "        for j in xrange(5):\n",
      "            h=self.RBM_positive(X)\n",
      "            v1=self.RBM_negative(h)\n",
      "            h1=self.RBM_positive(v1)\n",
      "            wint=self.RBM_update(X,v1,h,h1)\n",
      "        return wint"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import cPickle as pickle \n",
      "import random \n",
      "\n",
      "from sklearn.preprocessing import OneHotEncoder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.hmm import MultinomialHMM\n",
      "from sklearn.hmm import GaussianHMM\n",
      "from sklearn.hmm import GMMHMM\n",
      "    \n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def split_train_test(X, y, train_prop):\n",
      "    assert train_prop > 0 and train_prop < 1\n",
      "    N, P = X.shape\n",
      "    N_train = int(train_prop*N)\n",
      "    N_test = N - N_train\n",
      "    row_idx = [i for i in xrange(N)]\n",
      "    random.shuffle(row_idx)\n",
      "    train_idx = row_idx[:N_train]\n",
      "    test_idx = row_idx[N_train:]\n",
      "    X_train = X[train_idx,:]\n",
      "    y_train = y[train_idx]\n",
      "    X_test = X[test_idx,:]\n",
      "    y_test = y[test_idx]\n",
      "    return X_train, y_train, X_test, y_test, train_idx, test_idx\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = pd.read_pickle('data.pkl')\n",
      "#data1.drop(['WindGustM', 'WindChillM', 'HeatIndexM', 'PrecipM'], axis=1, inplace=True)\n",
      "#data = (data1 - data1.mean())/data1.std()\n",
      "\n",
      "y = data['Steam'].values\n",
      "X = data.drop('Steam', axis=1).values\n",
      "X_train, y_train, X_test, y_test, train_idx, test_idx = split_train_test(X,y,.20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'numpy.ndarray' object has no attribute 'values'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-34-b40587c26832>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "layers = [50, 1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NeuralNetwork(X_train, y_train, layers)\n",
      "nn.mainNN(X_train,y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "non-broadcastable output operand with shape (50,1) doesn't match the broadcast shape (50,50)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-36-57fbb2d256be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-24-3e1481f03825>\u001b[0m in \u001b[0;36mmainNN\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[1;32mprint\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mback_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m             \u001b[1;32mprint\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-24-3e1481f03825>\u001b[0m in \u001b[0;36mback_propagation\u001b[1;34m(self, X, y, z)\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_length\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[1;32mprint\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_length\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_length\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (50,1) doesn't match the broadcast shape (50,50)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "11754695533.5\n",
        "<type 'list'>\n",
        "e-shape (1,)\n",
        "z-shape (9312, 50)\n",
        "(50,)\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
        "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
        "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
        "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dates = matplotlib.dates.date2num(data.index[test_idx])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot_date(dates, lr_error)\n",
      "plt.xlabel('Date')\n",
      "plt.ylabel('Absolute error (Million pounds of steam/hour)')\n",
      "plt.title('Absolute error of steam demand prediction (LR)')\n",
      "plt.save('LR_error.png')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}